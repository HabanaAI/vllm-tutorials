{"GlobalRank": null, "LocalRank": 0, "Mode": "DynamicRange", "Nodes": {"model.layers.0.self_attn.qkv_proj": {"inputs": [[[4.59375]]], "params": {"weight": [[0.67578125]]}}, "model.layers.0.self_attn.o_proj": {"inputs": [[[0.77734375]]], "outputs": [[[0.51171875]], [[0.97265625]]], "params": {"weight": [[0.314453125]]}}, "model.layers.0.self_attn.attn.impl.matmul_qk": {"inputs": [[[2.828125]], [[13.8125]]]}, "model.layers.0.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[1.3359375]]]}, "model.layers.0.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[82.5]]]}, "model.layers.0.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[43.75]]]}, "model.layers.0.self_attn.attn.impl.k_cache": {"inputs": [[[13.8125]]]}, "model.layers.0.self_attn.attn.impl.v_cache": {"inputs": [[[1.3359375]]]}, "model.layers.0.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[22.625]], [[13.8125]], [[1.3359375]]], "outputs": [[[0.77734375]], [[1.0]]]}, "model.layers.0.mlp.gate_up_proj": {"inputs": [[[2.765625]]], "params": {"weight": [[0.59375]]}}, "model.layers.0.mlp.down_proj": {"inputs": [[[32.75]]], "outputs": [[[20.875]], [[1.2230847588139557e+30]]], "params": {"weight": [[0.62109375]]}}, "model.layers.1.self_attn.qkv_proj": {"inputs": [[[5.84375]]], "params": {"weight": [[0.40234375]]}}, "model.layers.1.self_attn.o_proj": {"inputs": [[[1.8984375]]], "outputs": [[[0.61328125]], [[1.0101590720568703e+30]]], "params": {"weight": [[0.51953125]]}}, "model.layers.1.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.640625]], [[14.0625]]]}, "model.layers.1.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.375]]]}, "model.layers.1.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[27.0]]]}, "model.layers.1.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[18.375]]]}, "model.layers.1.self_attn.attn.impl.k_cache": {"inputs": [[[14.0625]]]}, "model.layers.1.self_attn.attn.impl.v_cache": {"inputs": [[[2.375]]]}, "model.layers.1.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[13.5625]], [[14.0625]], [[2.375]]], "outputs": [[[1.8984375]], [[1.0]]]}, "model.layers.1.mlp.gate_up_proj": {"inputs": [[[18.5]]], "params": {"weight": [[1.1328125]]}}, "model.layers.1.mlp.down_proj": {"inputs": [[[1160.0]]], "outputs": [[[408.0]], [[624.0]]], "params": {"weight": [[0.6796875]]}}, "model.layers.2.self_attn.qkv_proj": {"inputs": [[[8.0]]], "params": {"weight": [[0.54296875]]}}, "model.layers.2.self_attn.o_proj": {"inputs": [[[1.3046875]]], "outputs": [[[0.859375]], [[9.111238689140399e+29]]], "params": {"weight": [[0.359375]]}}, "model.layers.2.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.4765625]], [[15.5]]]}, "model.layers.2.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.640625]]]}, "model.layers.2.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[29.25]]]}, "model.layers.2.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[21.0]]]}, "model.layers.2.self_attn.attn.impl.k_cache": {"inputs": [[[15.5]]]}, "model.layers.2.self_attn.attn.impl.v_cache": {"inputs": [[[2.640625]]]}, "model.layers.2.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[12.5625]], [[15.5]], [[2.640625]]], "outputs": [[[1.3046875]], [[1.0]]]}, "model.layers.2.mlp.gate_up_proj": {"inputs": [[[6.625]]], "params": {"weight": [[0.4609375]]}}, "model.layers.2.mlp.down_proj": {"inputs": [[[3.21875]]], "outputs": [[[1.3515625]], [[1.044821393156861e+30]]], "params": {"weight": [[0.53125]]}}, "model.layers.3.self_attn.qkv_proj": {"inputs": [[[7.03125]]], "params": {"weight": [[0.3046875]]}}, "model.layers.3.self_attn.o_proj": {"inputs": [[[1.859375]]], "outputs": [[[0.93359375]], [[1.1735671572425405e+30]]], "params": {"weight": [[0.2431640625]]}}, "model.layers.3.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.546875]], [[20.75]]]}, "model.layers.3.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.609375]]]}, "model.layers.3.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[40.25]]]}, "model.layers.3.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[24.875]]]}, "model.layers.3.self_attn.attn.impl.k_cache": {"inputs": [[[20.75]]]}, "model.layers.3.self_attn.attn.impl.v_cache": {"inputs": [[[2.609375]]]}, "model.layers.3.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[13.0]], [[20.75]], [[2.609375]]], "outputs": [[[1.859375]], [[1.0]]]}, "model.layers.3.mlp.gate_up_proj": {"inputs": [[[6.875]]], "params": {"weight": [[0.337890625]]}}, "model.layers.3.mlp.down_proj": {"inputs": [[[4.1875]]], "outputs": [[[1.0859375]], [[1.1735671572425405e+30]]], "params": {"weight": [[0.490234375]]}}, "model.layers.4.self_attn.qkv_proj": {"inputs": [[[6.40625]]], "params": {"weight": [[0.416015625]]}}, "model.layers.4.self_attn.o_proj": {"inputs": [[[1.484375]]], "outputs": [[[0.90234375]], [[1.7331160549995324e+30]]], "params": {"weight": [[0.3984375]]}}, "model.layers.4.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.5390625]], [[18.5]]]}, "model.layers.4.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.015625]]]}, "model.layers.4.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[34.25]]]}, "model.layers.4.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[19.0]]]}, "model.layers.4.self_attn.attn.impl.k_cache": {"inputs": [[[18.5]]]}, "model.layers.4.self_attn.attn.impl.v_cache": {"inputs": [[[2.015625]]]}, "model.layers.4.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[13.4375]], [[18.5]], [[2.015625]]], "outputs": [[[1.484375]], [[1.0]]]}, "model.layers.4.mlp.gate_up_proj": {"inputs": [[[6.9375]]], "params": {"weight": [[0.515625]]}}, "model.layers.4.mlp.down_proj": {"inputs": [[[3.90625]]], "outputs": [[[1.2734375]], [[1.7331160549995324e+30]]], "params": {"weight": [[0.470703125]]}}, "model.layers.5.self_attn.qkv_proj": {"inputs": [[[10.8125]]], "params": {"weight": [[1.109375]]}}, "model.layers.5.self_attn.o_proj": {"inputs": [[[1.8984375]]], "outputs": [[[1.125]], [[1.178518917399682e+30]]], "params": {"weight": [[0.447265625]]}}, "model.layers.5.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.453125]], [[20.125]]]}, "model.layers.5.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.828125]]]}, "model.layers.5.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[34.0]]]}, "model.layers.5.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[28.625]]]}, "model.layers.5.self_attn.attn.impl.k_cache": {"inputs": [[[20.125]]]}, "model.layers.5.self_attn.attn.impl.v_cache": {"inputs": [[[2.828125]]]}, "model.layers.5.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[13.375]], [[20.125]], [[2.828125]]], "outputs": [[[1.8984375]], [[1.0]]]}, "model.layers.5.mlp.gate_up_proj": {"inputs": [[[7.03125]]], "params": {"weight": [[0.357421875]]}}, "model.layers.5.mlp.down_proj": {"inputs": [[[4.40625]]], "outputs": [[[1.6953125]], [[1.178518917399682e+30]]], "params": {"weight": [[0.45703125]]}}, "model.layers.6.self_attn.qkv_proj": {"inputs": [[[8.75]]], "params": {"weight": [[0.3828125]]}}, "model.layers.6.self_attn.o_proj": {"inputs": [[[1.796875]]], "outputs": [[[1.21875]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.259765625]]}}, "model.layers.6.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.5]], [[21.0]]]}, "model.layers.6.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.453125]]]}, "model.layers.6.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[38.0]]]}, "model.layers.6.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[24.25]]]}, "model.layers.6.self_attn.attn.impl.k_cache": {"inputs": [[[21.0]]]}, "model.layers.6.self_attn.attn.impl.v_cache": {"inputs": [[[2.453125]]]}, "model.layers.6.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[13.8125]], [[21.0]], [[2.453125]]], "outputs": [[[1.796875]], [[1.0]]]}, "model.layers.6.mlp.gate_up_proj": {"inputs": [[[6.75]]], "params": {"weight": [[0.451171875]]}}, "model.layers.6.mlp.down_proj": {"inputs": [[[4.03125]]], "outputs": [[[1.765625]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.625]]}}, "model.layers.7.self_attn.qkv_proj": {"inputs": [[[9.1875]]], "params": {"weight": [[0.33203125]]}}, "model.layers.7.self_attn.o_proj": {"inputs": [[[1.9921875]]], "outputs": [[[1.375]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.3671875]]}}, "model.layers.7.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.59375]], [[18.5]]]}, "model.layers.7.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[3.421875]]]}, "model.layers.7.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[33.0]]]}, "model.layers.7.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[24.125]]]}, "model.layers.7.self_attn.attn.impl.k_cache": {"inputs": [[[18.5]]]}, "model.layers.7.self_attn.attn.impl.v_cache": {"inputs": [[[3.421875]]]}, "model.layers.7.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[15.5625]], [[18.5]], [[3.421875]]], "outputs": [[[1.75]], [[1.0]]]}, "model.layers.7.mlp.gate_up_proj": {"inputs": [[[6.90625]]], "params": {"weight": [[0.5859375]]}}, "model.layers.7.mlp.down_proj": {"inputs": [[[3.15625]]], "outputs": [[[2.5625]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.48046875]]}}, "model.layers.8.self_attn.qkv_proj": {"inputs": [[[10.625]]], "params": {"weight": [[0.498046875]]}}, "model.layers.8.self_attn.o_proj": {"inputs": [[[1.8828125]]], "outputs": [[[2.046875]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.404296875]]}}, "model.layers.8.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.3125]], [[19.125]]]}, "model.layers.8.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.4375]]]}, "model.layers.8.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[31.625]]]}, "model.layers.8.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[23.125]]]}, "model.layers.8.self_attn.attn.impl.k_cache": {"inputs": [[[19.125]]]}, "model.layers.8.self_attn.attn.impl.v_cache": {"inputs": [[[2.4375]]]}, "model.layers.8.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[12.75]], [[19.125]], [[2.4375]]], "outputs": [[[1.8828125]], [[1.0]]]}, "model.layers.8.mlp.gate_up_proj": {"inputs": [[[7.3125]]], "params": {"weight": [[0.546875]]}}, "model.layers.8.mlp.down_proj": {"inputs": [[[5.40625]]], "outputs": [[[2.5]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.578125]]}}, "model.layers.9.self_attn.qkv_proj": {"inputs": [[[9.8125]]], "params": {"weight": [[0.412109375]]}}, "model.layers.9.self_attn.o_proj": {"inputs": [[[1.859375]]], "outputs": [[[2.234375]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.498046875]]}}, "model.layers.9.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.8125]], [[19.25]]]}, "model.layers.9.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.78125]]]}, "model.layers.9.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[34.5]]]}, "model.layers.9.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[22.25]]]}, "model.layers.9.self_attn.attn.impl.k_cache": {"inputs": [[[19.25]]]}, "model.layers.9.self_attn.attn.impl.v_cache": {"inputs": [[[2.78125]]]}, "model.layers.9.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[15.6875]], [[19.25]], [[2.78125]]], "outputs": [[[1.859375]], [[1.0]]]}, "model.layers.9.mlp.gate_up_proj": {"inputs": [[[7.5]]], "params": {"weight": [[0.51171875]]}}, "model.layers.9.mlp.down_proj": {"inputs": [[[6.3125]]], "outputs": [[[2.421875]], [[1.1983259580282481e+30]]], "params": {"weight": [[0.5546875]]}}, "model.layers.10.self_attn.qkv_proj": {"inputs": [[[15.25]]], "params": {"weight": [[0.38671875]]}}, "model.layers.10.self_attn.o_proj": {"inputs": [[[2.078125]]], "outputs": [[[1.5234375]], [[8.120886657712095e+29]]], "params": {"weight": [[0.60546875]]}}, "model.layers.10.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.9296875]], [[17.25]]]}, "model.layers.10.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[3.0625]]]}, "model.layers.10.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[32.0]]]}, "model.layers.10.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[20.0]]]}, "model.layers.10.self_attn.attn.impl.k_cache": {"inputs": [[[17.25]]]}, "model.layers.10.self_attn.attn.impl.v_cache": {"inputs": [[[3.0625]]]}, "model.layers.10.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[16.375]], [[17.25]], [[3.0625]]], "outputs": [[[2.078125]], [[1.0]]]}, "model.layers.10.mlp.gate_up_proj": {"inputs": [[[7.875]]], "params": {"weight": [[0.57421875]]}}, "model.layers.10.mlp.down_proj": {"inputs": [[[10.6875]]], "outputs": [[[1.75]], [[8.120886657712095e+29]]], "params": {"weight": [[0.515625]]}}, "model.layers.11.self_attn.qkv_proj": {"inputs": [[[13.4375]]], "params": {"weight": [[0.431640625]]}}, "model.layers.11.self_attn.o_proj": {"inputs": [[[1.90625]]], "outputs": [[[1.78125]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.6328125]]}}, "model.layers.11.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.4921875]], [[21.875]]]}, "model.layers.11.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[2.796875]]]}, "model.layers.11.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[28.5]]]}, "model.layers.11.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[16.75]]]}, "model.layers.11.self_attn.attn.impl.k_cache": {"inputs": [[[21.875]]]}, "model.layers.11.self_attn.attn.impl.v_cache": {"inputs": [[[2.796875]]]}, "model.layers.11.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[12.25]], [[21.875]], [[2.796875]]], "outputs": [[[1.90625]], [[1.0]]]}, "model.layers.11.mlp.gate_up_proj": {"inputs": [[[7.8125]]], "params": {"weight": [[0.63671875]]}}, "model.layers.11.mlp.down_proj": {"inputs": [[[8.8125]]], "outputs": [[[2.9375]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.38671875]]}}, "model.layers.12.self_attn.qkv_proj": {"inputs": [[[12.8125]]], "params": {"weight": [[0.73828125]]}}, "model.layers.12.self_attn.o_proj": {"inputs": [[[1.8046875]]], "outputs": [[[2.671875]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.6953125]]}}, "model.layers.12.self_attn.attn.impl.matmul_qk": {"inputs": [[[1.9296875]], [[17.625]]]}, "model.layers.12.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[3.234375]]]}, "model.layers.12.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[20.75]]]}, "model.layers.12.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[15.4375]]]}, "model.layers.12.self_attn.attn.impl.k_cache": {"inputs": [[[17.625]]]}, "model.layers.12.self_attn.attn.impl.v_cache": {"inputs": [[[3.234375]]]}, "model.layers.12.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[15.1875]], [[16.875]], [[3.15625]]], "outputs": [[[1.8046875]], [[1.0]]]}, "model.layers.12.mlp.gate_up_proj": {"inputs": [[[7.71875]]], "params": {"weight": [[0.59765625]]}}, "model.layers.12.mlp.down_proj": {"inputs": [[[9.8125]]], "outputs": [[[2.96875]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.62890625]]}}, "model.layers.13.self_attn.qkv_proj": {"inputs": [[[14.0]]], "params": {"weight": [[0.51953125]]}}, "model.layers.13.self_attn.o_proj": {"inputs": [[[2.671875]]], "outputs": [[[3.1875]], [[1.6935019737424002e+30]]], "params": {"weight": [[0.462890625]]}}, "model.layers.13.self_attn.attn.impl.matmul_qk": {"inputs": [[[2.0]], [[19.625]]]}, "model.layers.13.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[3.75]]]}, "model.layers.13.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[24.375]]]}, "model.layers.13.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[13.4375]]]}, "model.layers.13.self_attn.attn.impl.k_cache": {"inputs": [[[19.625]]]}, "model.layers.13.self_attn.attn.impl.v_cache": {"inputs": [[[3.75]]]}, "model.layers.13.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[15.4375]], [[19.625]], [[3.75]]], "outputs": [[[2.671875]], [[1.0]]]}, "model.layers.13.mlp.gate_up_proj": {"inputs": [[[7.15625]]], "params": {"weight": [[0.6640625]]}}, "model.layers.13.mlp.down_proj": {"inputs": [[[13.125]]], "outputs": [[[3.765625]], [[1.6935019737424002e+30]]], "params": {"weight": [[0.451171875]]}}, "model.layers.14.self_attn.qkv_proj": {"inputs": [[[12.9375]]], "params": {"weight": [[0.56640625]]}}, "model.layers.14.self_attn.o_proj": {"inputs": [[[2.546875]]], "outputs": [[[5.21875]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.359375]]}}, "model.layers.14.self_attn.attn.impl.matmul_qk": {"inputs": [[[2.109375]], [[16.0]]]}, "model.layers.14.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[6.46875]]]}, "model.layers.14.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[20.0]]]}, "model.layers.14.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[16.75]]]}, "model.layers.14.self_attn.attn.impl.k_cache": {"inputs": [[[16.0]]]}, "model.layers.14.self_attn.attn.impl.v_cache": {"inputs": [[[6.46875]]]}, "model.layers.14.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[17.125]], [[15.75]], [[6.46875]]], "outputs": [[[2.546875]], [[1.0]]]}, "model.layers.14.mlp.gate_up_proj": {"inputs": [[[9.9375]]], "params": {"weight": [[0.7578125]]}}, "model.layers.14.mlp.down_proj": {"inputs": [[[16.875]]], "outputs": [[[6.09375]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.4453125]]}}, "model.layers.15.self_attn.qkv_proj": {"inputs": [[[8.75]]], "params": {"weight": [[0.486328125]]}}, "model.layers.15.self_attn.o_proj": {"inputs": [[[4.25]]], "outputs": [[[5.75]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.71875]]}}, "model.layers.15.self_attn.attn.impl.matmul_qk": {"inputs": [[[2.28125]], [[22.25]]]}, "model.layers.15.self_attn.attn.impl.matmul_av": {"inputs": [[[1.0]], [[5.09375]]]}, "model.layers.15.self_attn.attn.impl.batch2block_matmul": {"inputs": [[[1.0]], [[16.375]]]}, "model.layers.15.self_attn.attn.impl.block2batch_matmul": {"inputs": [[[1.0]], [[11.0625]]]}, "model.layers.15.self_attn.attn.impl.k_cache": {"inputs": [[[22.25]]]}, "model.layers.15.self_attn.attn.impl.v_cache": {"inputs": [[[5.09375]]]}, "model.layers.15.self_attn.attn.impl.fused_scaled_dot_product_attention": {"inputs": [[[17.5]], [[21.875]], [[5.09375]]], "outputs": [[[4.25]], [[1.0]]]}, "model.layers.15.mlp.gate_up_proj": {"inputs": [[[10.0]]], "params": {"weight": [[1.1953125]]}}, "model.layers.15.mlp.down_proj": {"inputs": [[[258.0]]], "outputs": [[[256.0]], [[1.6043702909138528e+30]]], "params": {"weight": [[0.828125]]}}, "lm_head": {"inputs": [[[36.25]]], "params": {"weight": [[0.361328125]]}}}}